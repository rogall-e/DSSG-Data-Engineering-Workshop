{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DSSG - Data Engineering Workshop \ud83e\udea0","text":"<p>Welcome to the Data Engineering Workshop organized by the Data Science for Social Good (DSSG) community. This workshop is designed to provide a hands-on introduction to the data engineering workflow and tools. The intended audience is data scientists and analysts who are interested in learning how to build simple data pipelines and data warehouses.</p>"},{"location":"#timetable","title":"Timetable","text":"Session Time Description Tool Giving Context and Getting to know each other 9:00 - 9:30 Introduce the workshop objectives and participants share their backgrounds and expectations Storing Data 9:30 - 10:15 Persisting data in a secure and queryable location for analytics purposes DuckDB Extracting and Loading 10:30 - 11:15 Transferring data from different systems to a centralized repository Airbyte Transforming 11:30 - 12:30 Shaping raw data from various sources into a unified view that can be interpreted by stakeholders dbt Making data accessible 12:45 - 13:15 Providing interpretation and data access to the rest of the organization Metabase Follow Up Questions and next steps 13:15 Participants can ask questions and receive guidance on recommended next steps and resources for further learning in data engineering"},{"location":"0_getstarted/developer-setup/","title":"Developer Setup","text":""},{"location":"0_getstarted/developer-setup/#gitpod-setup","title":"Gitpod Setup","text":""},{"location":"0_getstarted/developer-setup/#what-is-gitpod","title":"What is Gitpod?","text":"<p>Gitpod is a cloud-based IDE that allows you to develop software in your browser. It is based on Theia and VS Code. Gitpod has an open source program that offers 2,500 credits per month of free CDE usage but it also offers a free plan for private projects. Gitpod is a great tool for workshops and tutorials as it allows you to start with a pre-configured environment in your browser. This saves you the time to install and configure all the tools on your local machine.</p>"},{"location":"0_getstarted/developer-setup/#create-a-gitpod-account","title":"Create a Gitpod Account","text":"<p>To create a Gitpod account, go to gitpod.io and click on the \"Sign Up\" button in the top right corner. You can sign up with your GitHub, BitBucket or your GitLab account. The free plan is sufficient for this tutorial.</p>"},{"location":"0_getstarted/developer-setup/#fork-the-repo","title":"Fork the Repo","text":"<p>To fork the repo, click on the \"Fork\" button in the top right corner of the repo. This will create a copy of the repo in your GitHub account.</p>"},{"location":"0_getstarted/developer-setup/#open-the-repo-in-gitpod","title":"Open the Repo in Gitpod","text":"<p>To open the repo in Gitpod, go to Gitpod.io and click on the \"New Workspace\" button. </p> <p></p> <p>Here you can choose now repos that you can use in Gitpod.</p> <p></p> <p>This will take a few minutes. Once the workspace is ready, you will see the following screen:</p> <p></p> <p>Automatically Gitpod will set everything up for you. This will take a few minutes. Based on the .gitpod.yml file, Gitpod will install all the tools that are needed for this workshop. </p> <p>To stop the workspace, you can click on the \"Menu\" button in the top right corner. And in this menu you will <code>Gitpod: Stop Workspace</code>. This will stop the workspace from running but you can continue later where you left off with simply starting it again.</p>"},{"location":"0_getstarted/developer-setup/#optional-docker-and-docker-compose","title":"Optional: Docker and Docker-compose","text":"<p>For running this workshop locally, you need Docker, Docker-compose and a python environment. If you don't have Docker and Docker-compose installed, you can follow the instructions here and here.</p>"},{"location":"0_getstarted/developer-setup/#getting-started","title":"Getting Started","text":"<p>You need to fork the repo, click on the \"Fork\" button in the top right corner of the repo. This will create a copy of the repo in your GitHub account. And afterwards clone the repo.</p>"},{"location":"0_getstarted/developer-setup/#setup-python-environment","title":"Setup Python Environment","text":"<p>To setup the python environment you can use the provided requirements.txt file or poetry. </p>"},{"location":"0_getstarted/developer-setup/#setup-docker","title":"Setup Docker","text":"<p>To setup Docker, you can use the provided docker-compose.yaml file. Simply run <code>docker-compose up -d</code> in the root directory of the repo. </p> <p>This will start: - A Postgres Database - An Airbyte Server which you can access on http://localhost:8000 - A metabase instance which you can access on http://localhost:3000</p>"},{"location":"0_getstarted/expectations/","title":"Expectations","text":""},{"location":"0_getstarted/expectations/#we-will-learn-to","title":"We will learn to:","text":"<ul> <li>Set up a basic Analytical Database using DuckDB</li> <li>Read some data from various sources into our database with Airbyte</li> <li>Transform the data into a unified view with dbt</li> <li>Attach a visualization tool to the database using Metabase</li> </ul>"},{"location":"0_getstarted/expectations/#we-are-not-touching","title":"We are not touching:","text":"<ul> <li>Buildig a production-ready data pipeline.</li> <li>Setting up cloud infrastructure</li> <li>Orchestrating complex data pipelines with lots of dependencies</li> <li>Interacting with all the bells and whistles of the tools we will use. </li> </ul> <p>Warning</p> <p>We put this workshop together to help people get started with Data Engineering. Designing good material for workshops is quite some work.  This is the first iteration and we are happy to adapt the workshop interactively together with you.</p>"},{"location":"0_getstarted/usecase/","title":"The Yearly Pokemon Fundraiser","text":"<p>Learning things is fun! But learning things is also hard. And sometimes it is even harder to learn things that you already know. And most potentially you already know a lot of the things we will talk about today to some extend. What is new today might be that we try to glue everything together to solve one common use case of data engineering. And we will do so while solving the problem of the Pokemon Catastrophe.</p> <p>So lets start with what most work related things start with: A Problem. Honestly we do not care to much about Pokemon. But we do care about data engineering. And we want to learn about data. And luckily there is a lot of data about Pokemon. So lets use that data to learn about data engineering. And to make it a bit more interesting, we will not just learn about data engineering but we will also help the Pokemon to collect donations for a good cause.</p> <p>So lets hear a bit more about it </p> <p>Each year all Pokemon come together and collect money for a good cause. This year, the Pokemon agreed on collecting money to renovate all schools in the Kanto area.</p> <p>Last years fundraiser was a huge success and this year, a small group of Pokemon Scientists wants to find out a more about the donating behavior of the Pokemon. </p> <p>Their main research questions are: </p> <ul> <li>What type of Pokemon type is donating the most on average?</li> <li>On what day are Pokemon donating the most?</li> </ul> <p>The scientists collected some data, but struggle to get insights out of it. </p> <p>So let's jump right in and help them out!</p> <p> </p>"},{"location":"1_store/5_minutes_of_sql/","title":"SQL Syntax in 5 minutes. aka. DML, DDL, DCL...","text":"<p>When you look at the SQL Syntax you may notice that at least most simple things will be tackled quite similar all the time. Of course you still have freedom to do things a bit different in SQL. But I would claim that chances are much higher for two people to write the same SQL code for a given task compared to them writting the same Pandas code. </p> <p>SQL is split into three different categories:</p> <ul> <li>DDL (Data Definition Language): CREATE, ALTER, and DROP</li> <li>DML (Data Manipulation Language): SELECT, INSERT, UPDATE, and DELETE</li> <li>DCL (Data Control Language): GRANT and REVOKE</li> </ul>"},{"location":"1_store/5_minutes_of_sql/#1creating-a-table","title":"1.Creating a Table:","text":"SQLPandas <pre><code>CREATE TABLE Employees (\n    ID INT,\n    Name VARCHAR(50),\n    Age INT,\n    Group VARCHAR(50)\n);\n</code></pre> <pre><code>import pandas as pd\ndf = pd.DataFrame(columns=['ID', 'Name', 'Age', 'Group'])\n</code></pre>"},{"location":"1_store/5_minutes_of_sql/#2inserting-data","title":"2.Inserting Data:","text":"SQLPandas <pre><code>INSERT INTO Employees (ID, Name, Age, Group)\nVALUES (1, 'John', 30, 'A'),\n      (2, 'Jane', 25, 'B'),\n      (3, 'Smith', 35, 'A');\n</code></pre> <pre><code>data = {'ID': [1, 2, 3],\n        'Name': ['John', 'Jane', 'Smith'],\n        'Age': [30, 25, 35]}\n        'Group': ['A', 'B', 'A']}\ndf = pd.DataFrame(data)\n</code></pre>"},{"location":"1_store/5_minutes_of_sql/#3querying-data","title":"3.Querying Data:","text":"SQLPandas <pre><code>SELECT * FROM Employees;\n</code></pre> <pre><code>print(df)\n</code></pre>"},{"location":"1_store/5_minutes_of_sql/#4-aggregations","title":"4. Aggregations","text":"SQLPandas <pre><code>SELECT AVG(Age) as AverageAge, MAX(Age) as MaxAge\nFROM Employees\nGROUP BY Group;\n</code></pre> <pre><code>grouped_df = df.groupby('Group').mean()\n</code></pre>"},{"location":"1_store/5_minutes_of_sql/#5-permissions","title":"5. Permissions","text":"SQL <pre><code>GRANT SELECT, INSERT, UPDATE, DELETE ON employees TO user1;\n</code></pre> <p>=======</p>"},{"location":"1_store/data-evolution/","title":"The Evolution of Data Orgs","text":"<p>Disclaimer</p> <p>I am sketching a totally artificial and simplified evolution of stages for data in organisations. The goal is to show the different phases of data engineering and the different challenges that come with it.</p> Small Orgs - No Data PersonSmall Orgs - One Data PersonSmall/Mid Tech OrgsDecentralized Tech Orgs <ul> <li>Analytical needs are solved via direct communication</li> <li>Data is stored in a decentralized fashion, mostly in spreadsheets</li> </ul> <p></p> <ul> <li>Identified need for bringing data together and analyzing aggregates of it</li> <li>One centralized storage is set up</li> <li>Data is exported manually and imported into a central storage. Often still file based.</li> <li>The standardization is often driven from the cost side (e.g. Analyzing Marketing)</li> </ul> <p></p> <ul> <li>Tech Teams having their own databases to fullfill operations</li> <li>Data is already structured and can be retrieved from databases or APIs</li> </ul> <p></p> <ul> <li>Multiple Tech and Non Tech Domains having different data needs</li> <li>Data is not used for analytical purposes only.</li> <li>Products are build on top of data. Circular dependency between data and product.</li> <li>Teams own their own data pipelines and storage.</li> </ul> <p></p>"},{"location":"1_store/data-in-orgs/","title":"How organisations store data","text":""},{"location":"1_store/data-in-orgs/#there-will-be-spreadsheets","title":"There will be spreadsheets","text":"<p>All organisations store data in some kind of database! Most start with something like Excel or Google Sheets and most also never stop to use Spreadsheet. What changes over the years might be the amount of data and the growing urge to have more control over it! </p> <p>Following the urge of ordering the data chaos might be called Data Engineering. </p> <p> </p>"},{"location":"1_store/databases/","title":"Database Systems, Warehouses and Data Lakes","text":"<p>There are tons of tools out there that enable you to store and query data. Most tasks can be done with all of the solutions, which does not mean that it should be without consideration. Some things to take into account are:</p> <ul> <li>Transactional Databases: These are databases that are optimized for fast read and write operations and espcially consistency (ACID). They are usually used to store data that is used Backend Systems. Examples are Postgres, MySQL or DynamoDB (NoSQL).</li> <li>Analytical Databases: These are databases that are optimized for fast read operations. They are usually used to store data that is used for analysis. Examples are Redshift, BigQuery, Snowflake, or DuckDB.</li> <li>Data Lakes: Data lakes with Query Engines on top are usually used to store file based data that is then queried via SQL. They are not databases in the original sense but just provide an Query interface on top. They tend to give less guarantees when it comes to write consitency. Examples are S3, GCS, or HDFS with Presto, Hive, or Spark on top.</li> </ul> <p>Today we will focus on one data storing solution that we hope is a good start into data warehousing without the technical overhead: DuckDB.</p>"},{"location":"1_store/databases/#popular-data-storage-solutions","title":"Popular Data Storage Solutions","text":""},{"location":"1_store/duckdb/","title":"DuckDB","text":"<p>DuckDB is an open-source analytical SQL database management system designed for high performance and efficiency. It's built to excel in handling complex analytical queries on large volumes of data while maintaining a lightweight footprint. DuckDB is known for its exceptional speed in executing queries and its ability to operate seamlessly within various environments, from laptops to large-scale server setups.</p> <p>One of its standout features is its support for standard SQL queries, making it accessible to users familiar with SQL syntax. Additionally, DuckDB is optimized for read-heavy workloads, making it an ideal choice for data exploration, analytics, and research purposes.</p> <p>DuckDB also provides integration with popular data science tools like Jupyter Notebooks and Pandas, facilitating a smooth workflow for analysts and data scientists. Its compatibility with Jupyter Notebooks allows for an interactive and collaborative environment where users can harness DuckDB's power alongside their code and analysis.</p> <p>Moreover, DuckDB's integration with Pandas simplifies data manipulation and analysis. It enables the execution of SQL queries directly on Pandas DataFrames, providing a familiar interface for those comfortable with Pandas while leveraging DuckDB's speed and efficiency for data processing.</p> <p>Another notable aspect is that it can handle various data formats such as CSV, Parquet, and others. And let's you interact with these formats through SQL queries streamlines the process of querying and analyzing diverse data sources without the need for extensive data preprocessing.</p> <p>So DuckDB offers a user-friendly environment for data exploration, analysis, and manipulation. It's a great choice for those looking to leverage the power of SQL for data science and analytics.</p>"},{"location":"1_store/duckdb/#using-duckdb-to-load-a-csv-file","title":"Using DuckDB to load a csv file","text":"<p>You can query a csv file directly with DuckDB. This is very useful for data exploration and analysis.  <pre><code>SELECT * FROM 'flights.csv';\n</code></pre> or you can create a table from a csv file and query it later on.</p> <pre><code>CREATE TABLE ontime(FlightDate DATE, UniqueCarrier VARCHAR, OriginCityName VARCHAR, DestCityName VARCHAR);\nCOPY ontime FROM 'flights.csv' (AUTO_DETECT true);\n</code></pre> <p>you can even write the results of a query to a csv file.</p> <pre><code>COPY (SELECT * FROM ontime) TO 'flights.csv' WITH (HEADER 1, DELIMITER '|');\n</code></pre>"},{"location":"1_store/duckdb/#using-duckdb-with-pandas","title":"Using DuckDB with Pandas","text":"<p>You can use DuckDB with Pandas i.e. save queries into an pandas dataframe. </p> <pre><code>import duckdb\nimport pandas as pd\n\n# this will create a duckdb database in memory\ncon = duckdb.connect(database=':memory:', read_only=False)\n\n# this will create a table in the duckdb database\ncon.execute(\"CREATE TABLE ontime(FlightDate DATE, UniqueCarrier VARCHAR, OriginCityName VARCHAR, DestCityName VARCHAR);\")\n\n# this will copy the data from the csv file into the duckdb database\ncon.execute(\"COPY ontime FROM 'flights.csv' (AUTO_DETECT true);\")\n\n# this will query the data from the duckdb database and return a pandas dataframe\ndf = con.execute(\"SELECT * FROM ontime\").df()\n</code></pre> <p>But you can also use DuckDB to query a pandas dataframe directly. </p> <pre><code>import duckdb\nimport pandas as pd\n\n# load the csv file into a pandas dataframe\ndf = pd.read_csv('flights.csv')\n\n# this will query the data from the pandas dataframe and return a duckdb resultset\nduckdb.sql('Select * from df').fetchall()\n\n# create a duckdb database in memory\ncon = duckdb.connect(database=':memory:', read_only=False)\n\n# this will create a table in the duckdb database from the pandas dataframe\ncon.execute('CREATE TABLE df_table AS SELECT * FROM df')\n</code></pre> <p>If you want to try this out yourself and also see how it integrates into jupyter notebooks, have a look into the Intro to DuckDB juptyer notebook.</p>"},{"location":"1_store/sideeffects-of-storing-data/","title":"What are the side effects of storing data?","text":""},{"location":"1_store/sideeffects-of-storing-data/#security","title":"Security","text":"<p>When you store a lot of sensitive data in one easy to access place you also need to make sure only allowed people can access is.  The downsides of doing things centrally are that it is by far easier to make mistakes and expose data to the wrong people.</p>"},{"location":"1_store/sideeffects-of-storing-data/#gdpr","title":"GDPR","text":"<p>In the EU we have the GDPR. This is a law that regulates how data can be stored and processed. Have in mind that if you store data in a central place you need to make sure that you are compliant with the GDPR. Some examples of things you need to consider are:</p> <ul> <li>If you store customer data you need to make sure that you can delete it if a customer requests it.</li> <li>You need to make sure that you only store data that you actually need. You are not allowed to store data that you do not have valid business reason for.</li> </ul> <p>Note</p> <p>Luckily your are not requested to have automated procedures for all of this in place from the beginnnig. So as long as you have a process in place that allows you to do this manually you are fine. But you should consider this when you start building your data warehouse and move into automation as soon as possible.</p>"},{"location":"1_store/why-sql-is-great/","title":"Why SQL is great.","text":"<p>We know that you probably love your pandas. But today there will be no fluffy stuff for you. We will use SQL.</p> <ul> <li>SQL is the most widely used language for data analysis and it is also the most widely used language for data warehousing. </li> <li>It is a very powerful language and it is also very easy to learn. </li> <li>This also means that lots of other people outside of the data bubble have at least some knowledge of SQL. Software engineers, product managers, business analysts, and many others.</li> <li>It is mostly standardized. This means that you can use the same SQL code on different databases.</li> </ul> <p>We want things to be boring!</p> <p>Boring things have tons of benefits. They tend to stay boring in the same way which means they will work the same way in 5 or 10 or 20 years. At least chances are much higher.</p>"},{"location":"2_extract/connections/","title":"Connections, Jobs and Scheduling \u23f0","text":"<p>Most propably you do not want to get up every night at 3am to start your data ingestion. Luckily you do not have to! There are basically two ways to schedule your data ingestion:</p> <ol> <li>Scheduling the steps just based on time (e.g. using cron). Problem: The different steps have no idea about each other.</li> <li>Using an Orchestrator like Airflow or Prefect. Problem: You need to setup and maintain an orchestrator which can be quite complex in the beginning.</li> </ol> <p></p> <p>Tip</p> <p>If you are just getting started, you can use the first approach. Your jobs will not have many dependencies and most of the time everything will work. Most tools like Airbyte come with basic scheduling. If you want to build a production-ready data pipeline, you should use an orchestrator.</p>"},{"location":"2_extract/data-destinations/","title":"Data Destinations","text":"<p>Most of the time the destination for your data will be a database. But there are also other options like files or APIs. It could be that some of your stakeholders want the data delivered to another destination via a SFTP Server or an S3 Bucket. Options are endless.</p> <p>Note</p> <p>Most propably you will spend 95% of your time with your data warehouse or lake as destination. So we will focus on that for now. Luckily you are prepared to also move data out from there easily using e.g. Airbyte.</p> <p></p>"},{"location":"2_extract/data-sources/","title":"Data Sources","text":"<p>Almost everything that you gives you structured information in some way can act as a data source for your data ingestion. We already discussed spreadsheets but there is much more. You can get data from databases, APIs, files, and many more:</p> <ul> <li>Static Files (e.g. CSV, JSON, XML, ...)</li> <li>APIs (e.g. REST, GraphQL, ...) -&gt; That also serve something like JSON</li> <li>other Databases (e.g. Postgres, MySQL, ...)</li> </ul> <p></p> <p>Note</p> <p>There might be differences between internal and external data sources. For example, you might be able to access your internal databases directly, but you might have to use an API to access external data sources and have no idea about the implementation details of the data.</p>"},{"location":"2_extract/material/","title":"Extracting and Loading Data","text":"<p>Extracting and loading data is the process of getting data from a source and loading it into a target. The source can be a file, a database, an API, or any other source of data. The target can be a file, a database, or any other target that can store data.</p>"},{"location":"2_extract/material/#airbyte-intro","title":"Airbyte Intro","text":"<p>We will use Airbyte for the extraction and loading of data. Airbyte is an open-source data integration platform that helps you consolidate data from different sources and load it into a destination of your choice. It can be either self-hosted on your own infrastructure or you can use their cloud version. </p> <p>Airbyte follows a modular approach. It has a core platform that handles the orchestration of the data integration process and a set of connectors that handle the extraction and loading of data from different sources and to different destinations.</p> <p>If you are using Gitpod you can go to the <code>port</code> section of the terminal and click on the <code>Open Browser</code> button. This will open a new browser tab with the Airbyte UI. If you are running the tutorial locally you can go to <code>http://localhost:8000</code> to access the Airbyte UI after you started the docker containers.</p> <p>If you go to the UI it asks for a username and password. You can use <code>airbyte</code> as username and <code>password</code> as password. Afterwards you have to enter an email address and an organization name. You can use any email address and organization name you want (even test@test.com and test)</p> <p></p>"},{"location":"2_extract/material/#setting-up-the-airbyte-source","title":"Setting up the Airbyte Source","text":"<p>In our case we will connect two csv files as sources that are stored on github. But it would work similarly with any other data storages like for example an S3 bucket on AWS.</p> <p>To set up an source in airbyte click on the <code>Sources</code> tab and then on the <code>New Source</code> button. You will see a lot of connectors and a serach bar. Here you can look for <code>csv</code>. </p> <p></p> <p>Click on the <code>File</code> connector and you will see a form where you can enter the details for the source. Enter the following details:</p> <pre><code>Dataset Name: pokemon__masterdata\nFile Format: CSV\nURL: https://raw.githubusercontent.com/rogall-e/DSSG-Data-Engineering-Workshop/main/data/pokemon__masterdata.csv\n</code></pre> <p>(of course you can also take the link from your forked repository if you want to use your own data)</p> <p></p> <p>Afterwards click on the <code>Set up source</code> button to create the source.</p>"},{"location":"2_extract/material/#setting-up-the-airbyte-destination","title":"Setting up the Airbyte Destination","text":"<p>As a destination we will use a Postgres database that was started in gitpod automatically for you or you started with the docker compose command locally. You can access the database with the following credentials:</p> <pre><code>host: localhost\nport: 5432\ndatabase: pokemon\nuser: postgres\npassword: postgres\n</code></pre> <p>In Airbyte click on the <code>Destinations</code> tab and then on the <code>Create Destination</code> button. Select <code>Postgres</code> as destination and enter the credentials for the database (the password is under <code>Optional fields</code>). Afterwards click on the <code>Set up destination</code> button to create the destination.</p> <p></p>"},{"location":"2_extract/material/#setting-up-the-airbyte-connection","title":"Setting up the Airbyte Connection","text":"<p>Now we have to connect the source and the destination. Click on the <code>Connections</code> tab and then on the <code>Create Connection</code> button. Select the source </p> <p></p> <p>and the destination </p> <p></p> <p>you created before. Afterwards you get to the connection configuration page. Here you can select a schedule, a destination namespace and a prefix for the stream also you can choose the streams you want to sync and if your data should be normalized by airbyte or only the raw data should be loaded into the destination. In our case we will set our schedule type to manual so we have to trigger the sync by hand. Afterwards click on the <code>Set up connection</code> button to create the connection.</p> <p></p> <p>And now you can click on the <code>Sync Now</code> button to start the sync. You can see the progress/log of the sync in the <code>Job History</code> tab.</p> <p></p> <p>Now you have successfully extracted and loaded data from a source to a destination. </p>"},{"location":"3_transform/1_transformation_introduction/","title":"Transform Raw Data into something meaningful","text":"<p>While it's nice to have some raw data in place, in its current state, it won't help us to create insights and answer our most burning questions:</p> <ul> <li><code>How many Pokemon donated?</code></li> <li><code>How much money did we collect in total?</code></li> <li><code>Which pokemon type donated on average the most?</code></li> </ul> <p>To answer these questions, we need to crunch our data first and make it more useable for anaylsis. </p> <p>This might involve:</p> <ol> <li>Cleaning</li> <li>Joining</li> <li>Aggregating</li> </ol> <p>While \"crunching\" is a pretty cool word, a bit more suitable (and that's the topic of this section) might be: Data Transformation </p> <p></p> <pre><code>flowchart LR\n    RD(Raw Data) --- T[Data Transformation]:::empty\n    T --&gt; I(Value)\n    classDef empty width:175px,height:0px;</code></pre> <p></p> <p>Data Transformation is the key to create value from raw data!</p>"},{"location":"3_transform/2_data_transformation_workflow/","title":"A Data Transformation Workflow","text":""},{"location":"3_transform/2_data_transformation_workflow/#from-raw-to-insight","title":"From Raw to Insight","text":"<p>As mentioned before, there are certain types of data transformations like cleaning data, joining and aggregating it.</p> <p>In a data workflow, it makes sense to seperate these kind of operations and think in so called transformation layers. </p> <p>A typical layer architecture might look like this:</p> Layer Example Operations RAW - STAGING deduplicate data &amp; rename columns MART combine tables &amp; create calculations ANALYSIS aggregate tables &amp; create metrics <p>Let's take a look at an example:</p> RAWSTAGINGMARTANALYSIS <p>\"Oh no! There is a duplicate!\"</p> pokemon_id pokemon_name 1 Bulbasaur 2 Ivysaur 2 Ivysaur 3 Venusaur <p>\"That looks much nicer. Now add some more data!\"</p> pokemon_id pokemon_name 1 Bulbasaur 2 Ivysaur 3 Venusaur <p>\"Cool! I joined my donation data.\"</p> pokemon_id pokemon_name number_of_pokemon amount_donated 1 Bulbasaur 10 100.00 2 Ivysaur 20 200.00 3 Venusaur 5 50.00 <p>\"Wow! So many Pokemon donated! I have to share this with my friends.\"</p> number_of_pokemon amount_donated average_donation 35 350 10.00 <p>Using layers keeps your workflow structured!</p>"},{"location":"3_transform/2_data_transformation_workflow/#using-sql-to-take-care-of-transformations","title":"Using SQL to take care of transformations","text":"<p>One of the preferred ways of encoding transformation logic in the data world is SQL. </p> <p>As mentioned before, SQL is a very powerful and easy to learn language. So it makes sense to also use it for our transformations.</p> <p>Typically, we would write different SQL statements with different kind of transformations and execute them in the correct order:</p> <p></p> <pre><code>flowchart LR\n    SA(Raw Table A) --- Q1[SQL]\n    Q1 --&gt; T1([Table 1])\n\n    SB(Raw Table B) --- Q2[SQL]\n    Q2 --&gt; T2([Table 2])\n\n    T1 --- Q[SQL]\n    T2 --- Q\n    Q --&gt; T3([Table 3])</code></pre> <p></p> <p>As you can imagine, this can get quite complex and hard to maintain. Especially if you have many different raw tables and transformations.</p> <p>Some day your data lineage might look like this (each bubble represents a table): </p> <p>But worry no longer, there is a solution for this problem: DBT</p>"},{"location":"3_transform/3_dbt_introduction/","title":"DBT Introduction","text":""},{"location":"3_transform/3_dbt_introduction/#dbt-data-build-tool","title":"DBT - Data Build Tool","text":"<p>DBT is a tool, which helps us to manage our data transformation logic. </p> <p></p> <p>Everything you need are some <code>.sql</code> files, that hold your transformation logic, and DBT takes then care of the rest.</p> <p>While DBT has a lot of features, we want to highlight some of them here:</p> <ul> <li> <p>LINEAGE</p> <p>DBT has knowledge of your data lineage and knows what comes first and what next. It executes your files in the correct order.</p> </li> <li> <p>TESTING</p> <p>You can define and write tests and make sure, that your data quality is always at its best.</p> </li> <li> <p>DOCUMENTATION</p> <p>By adding column descriptions in your code, you can make sure, that everyone is one the same page about your data.</p> </li> <li> <p>JINJA </p> <p>DBT facilitates the use of the DRY principle (Don't Repeat Yourself) by using <code>jinja templating</code>.</p> </li> </ul> One word on the DRY principle (aka jinja templating). <p>DBT enables the use of jinja templating. This means, that you can use variables and loops in your SQL statements.</p> <p>This is especially useful, if you have to repeat the same logic over and over again.</p> <p>Here is an example:</p> <pre><code>-- This is a jinja variable\n-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \n{% set my_favorite_columns = ['column_a', 'column_b', 'column_c'] %}\n\n-- This is a jinja loop\n-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \nSELECT \n    {% for item in my_variable %}\n        item {%- if not loop.last %},{% endif %}\n    {% endfor %}\nFROM \n    my_schema.my_table\n\n-- This compiles to this:\n-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \nSELECT \n    column_a, \n    column_b, \n    column_c\nFROM\n    my_schema.my_table\n</code></pre> <p>More on that here: DBT Macros</p>"},{"location":"3_transform/4_dbt_basics/","title":"DBT Basics","text":""},{"location":"3_transform/4_dbt_basics/#a-quick-introduction-to-dbt","title":"A quick Introduction to DBT","text":"<p>DBT is pretty powerful and you can do a lot of fancy stuff with it. But let's keep it simple first and focus on the basic dbt project structure first:</p> <pre><code>my_dbt_project\n\u251c\u2500\u2500 dbt_project.yaml\n\u251c\u2500\u2500 profiles.yaml\n\u2514\u2500\u2500 models\n    \u251c\u2500\u2500 pokemon__model_a.sql\n    \u251c\u2500\u2500 pokemon__model_b.sql\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 schema.yaml\n</code></pre>"},{"location":"3_transform/4_dbt_basics/#dbt_projectyaml","title":"<code>dbt_project.yaml</code>","text":"<p>In the <code>dbt_project.yaml</code>, you can define the metadata of your project. For example:</p> <ul> <li>Naming</li> <li>How to store your tables?</li> <li>Defining directories</li> </ul> An example model <code>dbt_project.yaml</code> file <pre><code>name: 'pokemon_analytics'\nversion: '1.0.0'\nconfig-version: 2\n\nprofile: 'postgres'\n\nmodel-paths: [\"models\"]\n\ntarget-path: \"target\"  \nclean-targets:         \n- \"target\"\n- \"dbt_packages\"\n\nmodels:\n    pokemon_analytics:\n        materialized: table\n        intermediate:\n        +schema: intermediate\n        analytics:\n        +schema: analytics\n</code></pre>"},{"location":"3_transform/4_dbt_basics/#profilesyaml","title":"<code>profiles.yaml</code>","text":"<p>The <code>profiles.yaml</code> holds all of your connection information. Here you can add multiple outputs (for example database environments).</p> An example model <code>profiles.yaml</code> file <pre><code>postgres:\ntarget: dev\noutputs:\n    dev:\n    type: postgres\n    host: localhost\n    port: 5432\n    dbname: pokemon\n    user: postgre\n</code></pre>"},{"location":"3_transform/4_dbt_basics/#models","title":"<code>models/</code>","text":"<p>DBT uses the term model to define a sql logic. So each model represents a <code>.sql</code> file. In the models directory, you can also use subdirectories to organize your models (e.g. \"finance models\", \"marketing models\", etc.).</p> <p>Within your models directory, you can also create documentation and declare tests for your models. You can do that by simply creating another <code>.yaml</code> file and fill it with the required information.</p> An example model <code>schema.yaml</code> file <pre><code>version: 2\nmodels:\n- name: pokemon__model_a\ndescription: This table is super nice!\ncolumns:\n    - name: pokemon_id\n    desription: Unique identifier for a pokemon.\n    tests:\n        - unique\n        - not_null\n</code></pre> <p>With this .yaml we are saying, that the column <code>pokemon_id</code> in the model <code>pokemon__model_a</code> should be unique and not null. </p> <p>DBT will then automatically test this for us and throw an error, if the test fails.</p>"},{"location":"3_transform/4_dbt_basics/#ref","title":"<code>ref()</code>","text":"<p>As mentioned above, dbt is using <code>jinja templating</code> to boost your SQL development.</p> <p>Maybe the most famous use of jinja in DBT is the <code>ref()</code> function, which is used to reference other DBT models. </p> <pre><code>-- With the ref() function you can reference other DBT models:\n-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \nSELECT \n    *\nFROM \n    {{ ref(my_table) }} \n\n-- This compiles to this:\n-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- \nSELECT \n    *\nFROM\n    my_schema.my_table\n</code></pre>"},{"location":"3_transform/4_dbt_basics/#running-dbt","title":"Running dbt","text":"<p>There are multiple DBT commands to choose from. Here are the basic ones:</p> <pre><code>dbt run     # creates all models\ndbt test    # runs all tests\ndbt build   # runs both commands for each model\n</code></pre> <p>With <code>dbt run</code>, dbt checks your models directory and runs all of your models in the correct order.</p> <p>If you want to select a specific model or directory you can use <code>--select</code>:</p> <pre><code>dbt build --select 1_staging  # runs and tests all models in the 1_staging directory\ndbt build --select stg__pokemon__donations # rund and tests ONLY the model called stg__pokemon__donations\n</code></pre> <p>You can also create a webserver with all of your documentation:</p> <pre><code>dbt docs generate \ndbt docs serve\n</code></pre> <p>If you want to compile your jinja sql model, use:</p> <pre><code>dbt compile --select stg__pokemon__donations # compiles the models outputs the actual sql that runs on your database\n</code></pre>"},{"location":"4_analyze/dashboarding-tools/","title":"Analyze - 30min","text":"<p>This is the part where you get to play with the data. We will use a tool called Metabase to explore the data and answer some questions. This part is not our main focus as we assume that vizualizing data is not the main challenge for most of you.</p> <ul> <li>Most Dashboarding tools work quite similar. They are a front end to your Data Warehouse and most of the time come with a lot of connectors to connect to various Data Stores via SQL. </li> <li>They are able to adjust SQL queries dynamically, so you can filter and group your data in various ways. They also cache the data so that frequent queries are executed faster and do not load the same data again and again.</li> <li>Companies and people behind those tools want to make money to some extend. While this is totally fine, it is important to understand that this is also the reason why a lot of those tools have so many features while you actually only need a small subset of them. </li> <li>Popular Dashboarding tools are e.g. Metabase, Tableau, Looker, Power BI, Redash, Superset and many more. </li> </ul> What you need most of the time is the functionality to group line and bar charts in a dashboard and apply some dynamic filters on them. <p></p>"},{"location":"4_analyze/metabase/","title":"The Tool for today: Metabase \ud83d\udd0d","text":"<p>Metabase is a freemium open source. Tools There are tons of other tools out there. We just picked Metabase before we worked with it before, it has a SaaS offering, but also an open source version and it is quite easy to use.</p> <p> </p>"},{"location":"4_analyze/thin-analytics/","title":"Considerations when using Dashboarding tools","text":"<p>A lot of the Tools like Metabase, Tableau, PowerBI come with a rich set of functionality to not only visualize data, but also create on the fly aggrations, join data etc. To be able to do so they will generate and execute SQL for you under the hood. What sounds nice at first comes with a bunch of risks you should be aware of.</p> <ol> <li>The SQL that is generated is quite hard to maintain and keeping a structure like the one you learned about in the DBT Session today is almost impossible to maintain.</li> <li>The Dahboards itself are not versioned and it is hard to keep track of changes and who did what. So keeping the logic of the dashbaord as minimal as possible enables changing them easily later.</li> </ol> <p>You will benefit a lot from this approach after a few months. And your future colleagues will thank you for it. </p> <p>Consider moving as much processing logic as possible into your SQL Transformations instead of introducing another layer in your vizualizing tool.</p> <p>You might want to check out recent developments like the DBT Semantic Layer or Dashboarding Tools that integrate with DBT like LightDash.</p>"},{"location":"5_outlook/applying-your-skills/","title":"Applying your new skills. A little FAQ","text":"<p>I hope you enjoyed the workshop and learned something new. Now it is time to apply your new skills. Assume you have a small organisation you want to help out. Here are a few things to consider when you want to help them to structure their data needs:</p> <p>There is no silver bullet for data processing. Just try to keep things as simple as possible</p> We have no access to cloud infrastructure. What can we do? <p>Running things on Cloud Infrastructure definetly makes some things easier but it is not a must have. We tried to cover the most relevant categories of tools that you will need to build a data stack that helps teams to get insights from their data.</p> <p>Most of the tools we covered today have a Software as a Service (SaaS) offering.  This means you can use them without having to worry about infrastructure. Actually this is what we would recommend you to use in most cases, as the majority of organisations do not have the resources to maintain their own infrastructure.  Summing up all cost you will most propably end up much cheaper or even for free compared to running things on e.g. AWS.</p> <p>So check the tools you like to use. DBT Cloud for example comes free with one seat. Other Tools might also have speical free tier offerings for non profits. If nothing is listed on their Website just hit the sales team with a message. There is a good chance that they will help you out.</p> We have no data warehouse. What can we do? <p>As we mentioned before, a data warehouse is not a must have. It is just a tool that helps you to structure your data and make it accessible for your teams.  If you are just getting started you can also use a simple database like Postgres or MySQL.  The only thing you have to keep in mind is that you will have to do some more work to structure your data.  But this is also a good thing as you will learn a lot about the data you are working with.</p> I like Pandas and Python, do I need to do everything in SQL now? <p>No, you don't. We just think that SQL is a great tool to structure your data.  You can still use Pandas and Python for analysis.  But we think that you will benefit from a structured data warehouse in the long run.  And as you saw today, you can still use Python to build your data pipeline. DBT for example also supports Python Transformations for some use cases. So you can still use your favorite tools.</p>"},{"location":"5_outlook/learning-more/","title":"Where to go from here.","text":"<p>We know that half a day is a short time to get started with a potentially completely new topic. And we don't want to leave you alone with all the new things you learned today. So here are some pointers on where to go from here:</p>"},{"location":"5_outlook/learning-more/#learning-more-on-specific-tools-and-concepts","title":"Learning more on specific tools and concepts","text":""},{"location":"5_outlook/learning-more/#extract-and-load","title":"Extract and Load","text":"<ul> <li>Airbyte provides great guides on how to use their tool. You can find them here. Furthermore Airbyte follows a community approach for developing connectors. Developing a connector to e.g. a new API is very easy and you can find a guide here.  It will teach you a lot about the way data is shared and extracted from SaaS tools nowadays and how a lot of orgs are filling their data warehouses.</li> </ul>"},{"location":"5_outlook/learning-more/#transformations-with-dbt","title":"Transformations with DBT","text":"<ul> <li> <p>If you are more of a reader, we recommend you to take a look at the DBT docs. They are very well written and provide a lot of information on how to use dbt.</p> </li> <li> <p>There is also an Analytics Engineering Course provided by DBT</p> </li> </ul>"},{"location":"5_outlook/learning-more/#bootcamps-and-courses","title":"Bootcamps and Courses","text":"<p>If you want a bigger curriculum, we recommend you to take a look at the following courses:</p> <ul> <li>DataTalks.Club Data Engineering Bootcamp</li> <li>In Person Bootcamp (e.g. Neue Fische Bootcamp)</li> </ul>"},{"location":"5_outlook/learning-more/#exercises","title":"Exercises","text":"<p>We hope that you liked the Pokemon example we covered today. As you can imagine there are lots of things that we had no time to cover. So here are a few ideas for extending our data platform:</p> <ul> <li>Add a new source to Airbyte by direcly calling the Pokemon API. You can find the documentation here. To have a quick walkthrough of how to add a new source to Airbyte, you can take a look at the docs.</li> <li> <p>Add more transformations in dbt. You might want to try out the new semantic layer that was introduced in the recent version of dbt.</p> </li> <li> <p>There are cool new tools for data visualization.</p> </li> </ul> <p>It would be great if you point each other to nice resources using our workshops slack channel.</p>"}]}